{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Машинное обучение для текстов\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание <a id='Content'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Описание проекта](#Task_description)         \n",
    "[Описание данных проекта](#Data_description)     \n",
    "\n",
    "1. [Подготовка данных](#1)    \n",
    "    1.1 [Открытие и изучение данных](#1.1)    \n",
    "    1.2 [Очистка и лемматизация корпуса текстов ](#1.2)     \n",
    "    1.3 [Создание матрицы TF-IDF ](#1.3)    \n",
    "    1.4 [Создание Embeddings на базе BERT](#1.4)\n",
    "    \n",
    "2. [Обучение](#2)    \n",
    "    2.1 [Логистическая регрессия на TF-IDF](#2.1)    \n",
    "    2.2 [LightGBM на признаках TF-IDF](#2.2)    \n",
    "    2.3 [Логистическая регрессия на Embeddings из BERT](#2.3)    \n",
    "    2.4 [LightGBM на эмбеддингах из BERT](#2.4)\n",
    "\n",
    "3. [Выводы](#3)    \n",
    "\n",
    "[Чек-лист проверки](#Check_list)    \n",
    "[Записи для себя](#For_myself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Описание проекта](#Content) <a id='Task_description'></a>\n",
    "\n",
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. Клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо подготовить модель, которая отклассифицирует комментарии на позитивные и негативные. В наличии размеченный набор данных с классом \"токсичности\" правок.\n",
    "\n",
    "Мы стремимся построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "## [Описание данных](#Content) <a id='Data_description'></a>\n",
    "\n",
    "Данные записаны в файле `toxic_comments.csv`. \n",
    "\n",
    "Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Подготовка](#Content) <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, тема интересная и сложная с позиции объема машинных операций. В ходе проекта мы постараемся выбирать типы данных, которые не будут расходовать много памяти и максимально упрощать машинные операции. \n",
    "Почитав слак стало понятно, что выполнять работу лучше не на своих мощностях.\n",
    "\n",
    "Примерный порядок работы соответствует оглавлению.\n",
    "Открываем датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 [Открытие и изучение данных](#Content) <a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alt_path = 'https://code.s3.yandex.net/datasets/toxic_comments.csv'\n",
    "path = '/datasets/toxic_comments.csv'\n",
    "local_path = 'toxic_comments.csv'\n",
    "toxic_comments = pd.read_csv(local_path)\n",
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60467     \"\\nThe two of you both seem actively involved ...\n",
       "110669    Re:disruption \\n\\nlook i'm sorry it's nothing ...\n",
       "100037    As I said, an article can, and should, be edit...\n",
       "156749    The submission was deleted because it had no c...\n",
       "53978     Support – As you mention, the article title sh...\n",
       "132267    yo, , if u r going 2 impersonate me, at least ...\n",
       "71411     \"\\n\\nSitush , really sweet  to see  you are ta...\n",
       "14705     no abusing bots and stop being an idiot\\nthe p...\n",
       "115105                      the jews. Like, times a million\n",
       "125305    Are you a moderator? \\n\\nIf you aren't a moder...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.sample(10).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим две колонки в датасете как и сказано в описании:\n",
    "- Текст\n",
    "- его \"токсичность\" - целевой признак. Классы не сбалансированы    \n",
    "В тексте присутствуют спецсимволы, отвечающие за перенос и сдвиг каретки в начало строки. Местами присутствуют \"смайлы\", восклицания и прочие сочетания символов-выражения эмоций. \n",
    "Нам придется провести предобработку текста.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus1 = toxic_comments['text'].values.astype('U') # формируем корпус\n",
    "corpus = list(corpus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коротко, что мы видим по ошибкам или мусору в корпусе текстов и что мы точно должны устранить:\n",
    "\n",
    "| Ошибка       | Вид в тексте       | Исправление      |\n",
    "| -------------|:------------------:| ----------------:|\n",
    "| Знаки переноса строки    | \\n    | Заменяем на \" \" |\n",
    "| Апостроф с слешем     | Armenia\\\\'s |  Убираем слеш  |\n",
    "| Знаки цитат  |   \"\"       |  Убираем   |\n",
    "| URL линки    |   http://...       |  Убираем. полезной информации не несут   |\n",
    "| IP адреса    |   70.100.229.154       |  Убираем  |\n",
    "| Время и даты    |   04:28:57, August 19, 2007 (UTC)       |  Убираем  |\n",
    "| Наборы символов    |   ==,!.        |  Убираем  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 [Очистка и лемматизация корпуса текстов](#Content) <a id='1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного экспериментов \"для себя\" в целях проверить подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = toxic_comments.text[0]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the   page since I'm retired now.  .   .  .  \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rex1 = r'(\\d{2})[.-:](\\d{2}),\\w+(\\d{2}),(\\d{4})(\\(UTC\\))'\n",
    "rex2 = r'(\\d+)[/.-](\\d{2})[/.-](\\d{4})$'\n",
    "rex3 = r'[(\\d+)(\\:)]|(\\btalk\\b|\\bUTC\\b)|((\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3}))|(\\n)'\n",
    "rex4 = r'(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})'\n",
    "rex5 = r'(\\btalk\\b|\\bUTC\\b)'\n",
    "re.sub(rex3,' ',string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окончание экспериментов))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пишем готовую функцию, которая:\n",
    "1. Очищает текст от \"лишней\" информации, не имеющей отношения к \"позитивности\":\n",
    "    - числа, даты, время, IP\n",
    "    - Ссылки. Все равно контекст находится в рамках предложения\n",
    "    - знаки и спецсимволы\n",
    "2. Лемматизации текста\n",
    "\n",
    "Декомпозицией слов, склеенных через апостроф пренебрежем    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from pymystem3 import Mystem #После тестов отказался от конструкции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 378 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pnedviga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pnedviga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.9 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pnedviga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):    \n",
    "    regex =r'[^a-zA-Z]|(\\bUTC\\b|(http://\\S+))' # r'[^a-zA-Zа-яА-ЯёЁ]'\n",
    "    cleaned_text = re.sub(regex, ' ', text.lower())    \n",
    "    cleaned_text = cleaned_text.split()\n",
    "    cleaned_text = \" \".join(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "'''\n",
    "Создаем объект класса WordNetLemmatizer. Я его вынес за функцию, в которой он используется. \n",
    "Иначе функция создает его отдельно каждый раз, сильно замедляя работу алгоритма\n",
    "'''\n",
    "m = WordNetLemmatizer() \n",
    "def lemmatize(text):    \n",
    "    lemm_list = m.lemmatize(text)\n",
    "    lemm_text = \"\".join(lemm_list)        \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "Juelz Santanas Age\n",
      "\n",
      "In 2002, Juelz Santana was 18 years old, then came February 18th, which makes Juelz turn 19 making songs with The Diplomats. The third neff to be signed to Cam's label under Roc A Fella. In 2003, he was 20 years old coming out with his own singles \"\"Santana's Town\"\" and \"\"Down\"\". So yes, he is born in 1983. He really is, how could he be older then Lloyd Banks? And how could he be 22 when his birthday passed? The homie neff is 23 years old. 1983 - 2006 (Juelz death, god forbid if your thinking about that) equals 23. Go to your caculator and stop changing his year of birth. My god.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'juelz santanas age in juelz santana was years old then came february th which makes juelz turn making songs with the diplomats the third neff to be signed to cam s label under roc a fella in he was years old coming out with his own singles santana s town and down so yes he is born in he really is how could he be older then lloyd banks and how could he be when his birthday passed the homie neff is years old juelz death god forbid if your thinking about that equals go to your caculator and stop changing his year of birth my god'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = corpus[15]\n",
    "print(test_text)\n",
    "clear_text(test_text) # Проверяем работоспособность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      " Snowflakes are NOT always symmetrical! \n",
      "\n",
      "Under Geometry it is stated that \"\"A snowflake always has six symmetric arms.\"\" This assertion is simply not true! According to Kenneth Libbrecht, \"\"The rather unattractive irregular crystals are by far the most common variety.\"\" http://www.its.caltech.edu/~atomic/snowcrystals/myths/myths.htm#perfection Someone really need to take a look at his site and get FACTS off of it because I still see a decent number of falsities on this page. (forgive me Im new at this and dont want to edit anything)\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snowflakes are not always symmetrical under geometry it is stated that a snowflake always has six symmetric arms this assertion is simply not true according to kenneth libbrecht the rather unattractive irregular crystals are by far the most common variety someone really need to take a look at his site and get facts off of it because i still see a decent number of falsities on this page forgive me im new at this and dont want to edit anything'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = corpus[22]\n",
    "print(test_text)\n",
    "clear_text(test_text) # Проверяем работоспособность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает.    \n",
    "Я определю единую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clear_and_lemmatize_text(text):  \n",
    "    \n",
    "    regex =r'[^a-zA-Z\\']|(\\butc\\b|(http://\\S+))' # r'[^a-zA-Zа-яА-ЯёЁ]'\n",
    "    cleaned_text = re.sub(regex, ' ', text.lower())    \n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(cleaned_text)\n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим функцию тега, описывающую часть речи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"are\", 'v')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feet', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(['feet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_and_lemmatize_text_with_pos_tag(text):    \n",
    "    regex =r'[^a-zA-Z]|(\\butc\\b|(http://\\S+))' # r'[^a-zA-Zа-яА-ЯёЁ]'\n",
    "    cleaned_text = re.sub(regex, ' ', text.lower())    \n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(cleaned_text)\n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for row in corpus[0:1000]:\n",
    "#     print(row)\n",
    "    clear_and_lemmatize_text_with_pos_tag(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если оценивать примерно общее время, то видим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.66666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160*40/60 # минуты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100+ минут ожидаемое время. Терпимо.\n",
    "\n",
    "Операция в ячейке ниже проделана один раз. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# corpus_cleaned = []\n",
    "\n",
    "# for row in corpus:\n",
    "#     corpus_cleaned.append(clear_and_lemmatize_text_with_pos_tag(row))   \n",
    "# corpus_cleaned\n",
    "# d = {'text': corpus_cleaned}\n",
    "# lemmas = pd.DataFrame(data = d).to_csv('lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: \n",
      " Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Очищенный и лемматизированный текст: \n",
      " explanation why the edits make under my username hardcore metallica fan be revert they weren t vandalism just closure on some gas after i vote at new york doll fac and please don t remove the template from the talk page since i m retire now\n"
     ]
    }
   ],
   "source": [
    "print(\"Исходный текст: \\n\", corpus[0])\n",
    "print(\"Очищенный и лемматизированный текст: \\n\", clear_and_lemmatize_text_with_pos_tag(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обработать тот же текст без тегов алгоритмом из тренажера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "def lemm_clear_text(text):\n",
    "    word_list = tknzr.tokenize(text) # разбиваем входной текст на токены(слова)\n",
    "    lemm_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list]) # лемматизируем каждое слово и объединяем в строку\n",
    "    clear_list = re.sub(r'[^A-Za-z\\']', ' ', lemm_text) #оставляем только латинские символы\n",
    "    return \" \".join(clear_list.split()) #функция возвращает очищенный и лемматизированный текст в виде строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# применим функцию к нашим комментариям и создадим столбец, хранящий лемматизированные предложения\n",
    "corpus_cleaned_v2 = list(toxic_comments['text'].str.lower().apply(lambda x: lemm_clear_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: \n",
      " Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Очищенный и лемматизированный текст: \n",
      " explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalism just closure on some gas after i voted at new york doll fac and please don't remove the template from the talk page since i'm retired now\n"
     ]
    }
   ],
   "source": [
    "print(\"Исходный текст: \\n\", corpus[0])\n",
    "print(\"Очищенный и лемматизированный текст: \\n\", corpus_cleaned_v2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': corpus_cleaned_v2}\n",
    "pd.DataFrame(data = d).to_csv('lemmas_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 [Создание матрицы TF-IDF](#Content) <a id='1.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем ранее записанный очищенный корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_lemmas = pd.read_csv('lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text\n",
       "0           0  explanation why the edits make under my userna...\n",
       "1           1  d aww he match this background colour i m seem...\n",
       "2           2  hey man i m really not try to edit war it s ju...\n",
       "3           3  more i can t make any real suggestion on impro...\n",
       "4           4  you sir be my hero any chance you remember wha..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_lemmas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем в список в кодировке Unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = list(clear_lemmas['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lemmatized_corpus` - очищенный корпус текстов далее предстоит преобразовать в матричную форму, с которой может работать машина. \n",
    "\n",
    "Проэкспериментируем с размерностями биграмм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем число уникальных биграмм (пункт выполнен ради собственного интереса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (159571, 1870582)\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "# создайте n-грамму n_gramm, для которой n=2\n",
    "n_gramm = count_vect.fit_transform(lemmatized_corpus)\n",
    "print(\"Размер:\", n_gramm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чудеса размерности матрицы... >1.800.000 уникальных биграмм. Уберем стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pnedviga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it', 'myself', 'yourselves', 'from', \"couldn't\", 'will', 'your', \"you're\", 'very', 'you', 'same', 'these', 'off', 'during', 'herself', \"you'll\", 'with', 'them', 'been', \"you've\", 'because', 'i', \"that'll\", 'how', \"aren't\", 'll', 'which', 'couldn', 'an', 'before', \"haven't\", 'can', 'theirs', \"mightn't\", 'after', 'doesn', 'needn', 'then', 'most', 'out', 'aren', 'both', 'my', \"it's\", 's', 'was', 'we', 'shan', 'do', 'over', 'now', 'above', 'does', 'being', 'me', 'her', \"should've\", 'wasn', 'themselves', 'this', 'of', 'again', 'm', 'mightn', 'no', 'to', \"didn't\", 'just', 'they', 'as', 'yourself', 'shouldn', \"isn't\", 'had', \"won't\", 'here', 'a', 'has', 'have', 'isn', 'by', \"don't\", 'ma', 'below', 'didn', 'are', 'ourselves', 'having', 'wouldn', 'in', 'up', 'about', 'down', 'his', 'be', \"weren't\", 'through', 'not', \"she's\", 'once', 'yours', 'while', 'doing', 'only', \"needn't\", 'each', 'he', 'more', 'such', 'that', \"hasn't\", \"wouldn't\", 'haven', 'until', 'd', 'himself', 'were', \"doesn't\", \"shouldn't\", 'for', 'those', 'their', 'should', \"you'd\", 'won', 'itself', 'am', \"hadn't\", \"shan't\", 'if', 'against', 'but', 'don', 'o', 'when', 'hadn', 'so', 'between', 'where', 'and', 'few', 'mustn', 'is', 'at', 'any', 'our', 't', 're', 'she', 'nor', 'some', 'its', 'on', 'other', 'hers', 'under', 'y', 'what', 'whom', 'hasn', 'the', 'why', 'did', 'there', 'weren', 've', \"wasn't\", 'own', 'him', 'than', 'further', 'who', 'ain', 'ours', 'or', \"mustn't\", 'into', 'too', 'all'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (159571, 2300282)\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_vect = CountVectorizer(stop_words=stop_words,ngram_range=(2, 2))\n",
    "n_gramm = count_vect.fit_transform(lemmatized_corpus)\n",
    "print(\"Размер:\", n_gramm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность матрицы повысилась (было `(159571, 1870582)`, стало `(159571, 2300282)`). Интересно. Причина должна быть в том, что число сочетаний с стоп-словами меньше ввиду частоты употребления этих слов в биграмме. Убрав их (стоп-слова) мы увеличили число \"уникальных\" сочетаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> Круто, что разобрался с этим)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, после небольшой исследовательской части, делим датасет на тестовую и валидационную выборку для формирования матрицы признаков TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corp_v1_train, corp_v1_test, corp_v2_train,corp_v2_test, toxic_train, toxic_test)= (train_test_split(lemmatized_corpus, \n",
    "                                                                                                        corpus_cleaned_v2,\n",
    "                                                                                                        toxic_comments.toxic,\n",
    "                                                                                                        test_size=0.2, \n",
    "                                                                                                        random_state=124211))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к вычислению TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer_v1 = TfidfVectorizer(stop_words=stop_words,min_df = 2)\n",
    "TfidfVectorizer_v2 = TfidfVectorizer(stop_words=stop_words,min_df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему 2 шт. как пороговое значение? Ответить честно не могу. Идея такая - у нас выборка делится на 2 части: обучающую и тестовую. Соответственно шансов попасть в тестовую выборку при частоте <2 у признака нет.  В таком случае несет ли он смысловую нагрузку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы train: (127656, 51132)\n",
      "Размер матрицы test: (31915, 51132)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_v1_train = TfidfVectorizer_v1.fit_transform(corp_v1_train)\n",
    "tf_idf_v1_test = TfidfVectorizer_v1.transform(corp_v1_test)\n",
    "print(\"Размер матрицы train:\", tf_idf_v1_train.shape)\n",
    "print(\"Размер матрицы test:\", tf_idf_v1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо. Уникальных слов всего чуть больше 51 тысячи... Жить стало попроще. На основе данных признаков можно будет приступать к обучению в [главе 2.1](#2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое проделываем с другой версией корпуса текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы train: (127656, 57478)\n",
      "Размер матрицы test: (31915, 57478)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_v2_train = TfidfVectorizer_v2.fit_transform(corp_v2_train)\n",
    "tf_idf_v2_test = TfidfVectorizer_v2.transform(corp_v2_test)\n",
    "print(\"Размер матрицы train:\", tf_idf_v2_train.shape)\n",
    "print(\"Размер матрицы test:\", tf_idf_v2_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число признаков уже побольше: 57,5 тыс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 [Создание Embeddings на базе BERT](#Content) <a id='1.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насколько информирует нас тренажер, модель BERT позволяет нам закодировать слова с учетом их контекста и смысловой нагрузки. За такую точность нам приходится платить скоростью работы алгоритма. \n",
    "\n",
    "Поскольку возможности компьютера не безграничны, нам надо облегчить задачу для машины насколько это возможно:\n",
    "1. Брать облегченные модели BERT\n",
    "2. Ограничить размер батча в рамках одной итерации\n",
    "3. Использовать на \"полную\" мощность имеющегося \"железа\". Полагаю видеокарта nVidia будет отличным подспорьем (https://www.kaggle.com/atulanandjha/distilbert-on-gpu-tutorial-classification-problem)\n",
    "4. Облачные платформы (для себя ставлю в планы). Говорят kaggle дает несколько часов облачных вычислений \"в подарок\".\n",
    "\n",
    "Начнем с использования ядер GPU. Мы не будем ждать долго\n",
    "Приступаем    \n",
    "https://pytorch.org/?utm_source=Google&utm_medium=PaidSearch&utm_campaign=%2A%2ALP+-+TM+-+General+-+HV+-+RU&utm_adgroup=PyTorch+Installation&utm_keyword=pytorch%20installation&utm_offering=AI&utm_Product=PyTorch&gclid=EAIaIQobChMIrbrkhqSf6gIVDhsYCh3AcA-zEAAYASAAEgJM2fD_BwE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch===1.5.1 in c:\\users\\pnedviga\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: torchvision===0.6.1 in c:\\users\\pnedviga\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: future in c:\\users\\pnedviga\\anaconda3\\lib\\site-packages (from torch===1.5.1) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pnedviga\\anaconda3\\lib\\site-packages (from torch===1.5.1) (1.19.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\pnedviga\\anaconda3\\lib\\site-packages (from torchvision===0.6.1) (7.0.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\users\\\\pnedviga\\\\anaconda3\\\\lib\\\\site-packages\\\\pyodbc-4.0.27.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "pip install torch===1.5.1 torchvision===0.6.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее идем согласно статье: https://habr.com/ru/post/498144/\n",
    "\n",
    "Импортируем библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import transformers as ppb # pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "# Загрузка предобученной модели/токенизатора \n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что там делает с предложениями токенизатор...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [101, 7526, 2339, 1996, 10086, 2015, 2081, 210...\n",
       "1    [101, 1040, 1005, 22091, 2860, 999, 2002, 3503...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments['text'][0:2].apply(\n",
    "  lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_comments['text'][0:2].apply(\n",
    "  lambda x: tokenizer.encode(x, add_special_tokens=True))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_comments['text'][0:2].apply(\n",
    "  lambda x: tokenizer.encode(x, add_special_tokens=True))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый текст токенизируется в отдельный вектор. Каждое слово получает свой номер токена. Это первый этап."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем тексты. Но мы возьмем не всю выборку, а сделаем даунсемплинг для баланса классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26897\n",
       "1     3103\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comment_reduced = toxic_comments.sample(n=30000,random_state=1)\n",
    "toxic_comment_reduced.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы наши токены не получились длиннее 512 позиций ставим ограничитель. Иначе выдается предупреждение об общем количестве векторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized = toxic_comment_reduced['text'].apply(\n",
    "  lambda x: tokenizer.encode(x, add_special_tokens=True,max_length = 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводим все вектора к одному размеру. У нас он как раз 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 512)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак получили прямоугольную матрицу с токенами на каждый пост. Часть из постов обрезана. Таковы ограничения модели и моего железа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 512)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим маску для важных токенов\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabbb1c8211c4b2680b5aa1cd09ab9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(100 // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня сокрость 2 предложения в секунду. На всю выборку уйдет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ожидаемое время в часах работы машины 22.22222222222222\n"
     ]
    }
   ],
   "source": [
    "print(f'Ожидаемое время в часах работы машины {(160000/2/60/60)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не устраивает! Попробуем поработать с графическим ядром."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 27 18:46:53 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 441.45       Driver Version: 441.45       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   69C    P8    N/A /  N/A |     75MiB /  4096MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1356    C+G   Insufficient Permissions                   N/A      |\n",
      "|    0     67556    C+G   ...xperience\\NVIDIA GeForce Experience.exe N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 512])\n",
      "<class 'torch.Tensor'> torch.Size([20, 512])\n",
      "<class 'torch.Tensor'> torch.Size([20, 512, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([30522, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768])\n",
      "<class 'torch.Tensor'> torch.Size([20, 768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "#initiating Garbage Collector for GPU environment setup\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU=True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('using device: cuda')\n",
    "    \n",
    "else:\n",
    "    print('using device: cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = USE_GPU and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeadb3f1eab42bdb2acdff31ef8d536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 8.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = []\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    batch_size = 10    \n",
    "    model.to(device)\n",
    "    for i in notebook.tqdm(range(100 // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device) #LongTensor\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device) #LongTensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ожидаемое время в часах работы машины 2.2222222222222223\n"
     ]
    }
   ],
   "source": [
    "print(f'Ожидаемое время в часах работы машины {(160000/100*5/60/60)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features_BERT'></a>\n",
    "Это победа, друзья! Значительное увеличение производительности! Почувствуйте разницу! Большую скорость даст только производительная видеокарта или облачные вычисления. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы работаем сейчас на части общей выборки. Думаю число текстов в 30000 вполне должно хватить, чтобы оценить точность потенциальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4d1096a13148ff9ee8a6958ef4dd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1h 21min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = []\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    batch_size = 20    \n",
    "    model.to(device)\n",
    "    for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device) #LongTensor\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device) #LongTensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_BERT = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подготовили признаки для обучения модели: эмбеддинги в количестве 10 тыс. Остается проверить, насколько хорошо они отражают реальное содержимое текста. Об этом мы узнаем в продолжении в [главе 2.3](#2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> Очень круто, что разобрался как использовать Берт. Это очень классный уровень</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [Обучение](#Content) <a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 [Логистическая регрессия на признаках TF-IDF](#Content) <a id='2.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7426067907995619"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_regression = LogisticRegression(class_weight = 'balanced',max_iter=200)\n",
    "log_regression.fit(tf_idf_v1_train, toxic_train)\n",
    "f1_score(toxic_test,log_regression.predict(tf_idf_v1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7472406181015452"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_regression_v2 = LogisticRegression(class_weight = 'balanced',max_iter=400)\n",
    "log_regression_v2.fit(tf_idf_v2_train, toxic_train)\n",
    "f1_score(toxic_test,log_regression_v2.predict(tf_idf_v2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заданная точность метрики **НЕ достигнута**. Но мы обучим и другие модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 [LightGBM на признаках TF-IDF](#Content) <a id='2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMCl = LGBMClassifier(random_state = 12345, max_depth = 10, n_estimators = 50, metric = 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=10,\n",
       "               metric='f1_score', min_child_samples=20, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=50, n_jobs=-1, num_leaves=31,\n",
       "               objective=None, random_state=12345, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LGBMCl.fit(tf_idf_v2_train, toxic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6392099180500105"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(toxic_test,LGBMCl.predict(tf_idf_v2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока что печально проигрывает линейной регрессии.\n",
    "\n",
    "Далее немного оптимизируем подход к подбору гиперпараметров:\n",
    "https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e\n",
    "https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials, space_eval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "lgb_clf_params = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(5, 33, 3, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
    "    'n_estimators':     100,\n",
    "}\n",
    "lgb_fit_params = {\n",
    "    'eval_metric': 'binary',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "lgb_para = dict()\n",
    "lgb_para['clf_params'] = lgb_clf_params\n",
    "lgb_para['fit_params'] = lgb_fit_params\n",
    "lgb_para['loss_func' ] = lambda y, pred: (-f1_score(y, pred)) # np.sqrt(mean_squared_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOpt(object):\n",
    "\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "    \n",
    "    def lgb_reg(self, para):\n",
    "        reg = lgb.LGBMClassifier(**para['clf_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def train_reg(self, reg, para):\n",
    "        reg.fit(self.x_train, self.y_train,\n",
    "                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
    "                **para['fit_params'])\n",
    "        pred = reg.predict(self.x_test)\n",
    "        loss = para['loss_func'](self.y_test, pred)\n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем на тренировочную/валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tf_idf_v1_tr, tf_idf_v1_val, tf_idf_v2_tr,tf_idf_v2_val, toxic_tr, toxic_val) = (train_test_split(tf_idf_v1_train,\n",
    "                                                                                             tf_idf_v2_train,\n",
    "                                                                                             toxic_train,\n",
    "                                                                                             test_size=0.2, \n",
    "                                                                                             random_state=124211))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее загружаем данные и начинаем оптимизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obj_v1 = HPOpt(tf_idf_v1_tr, \n",
    "            tf_idf_v1_val,\n",
    "            toxic_tr,\n",
    "            toxic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:36<00:00,  9.60s/trial, best loss: -0.768667957822251]\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb_opt_v1 = obj_v1.process(fn_name='lgb_reg', \n",
    "                      space=lgb_para, \n",
    "                      trials=Trials(), \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получены следующие оптимальные параметры по TF-IDF версии 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7000000000000002,\n",
       " 'learning_rate': 0.3,\n",
       " 'max_depth': 29,\n",
       " 'min_child_weight': 2,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.8329239159623381}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_params_v1 = space_eval(lgb_clf_params, lgb_opt_v1[0])\n",
    "optimum_params_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMCl_opt_v1 = LGBMClassifier(**optimum_params_v1)#random_state = 12345, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=0.7000000000000002, importance_type='split',\n",
       "               learning_rate=0.3, max_depth=29, min_child_samples=20,\n",
       "               min_child_weight=2, min_split_gain=0.0, n_estimators=100,\n",
       "               n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=0.8329239159623381, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LGBMCl_opt_v1.fit(tf_idf_v1_train, toxic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7682119205298014"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(toxic_test,LGBMCl_opt_v1.predict(tf_idf_v1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уровень метрики f1 на тестовой выборке достигнут! `0.768`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем тот же метод на TF-IDF_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obj_v2 = HPOpt(tf_idf_v2_tr, \n",
    "            tf_idf_v2_val,\n",
    "            toxic_tr,\n",
    "            toxic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:35<00:00,  9.53s/trial, best loss: -0.7706382051840557]\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb_opt_v2 = obj_v2.process(fn_name='lgb_reg', \n",
    "                      space=lgb_para, \n",
    "                      trials=Trials(), \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получены следующие оптимальные параметры для версии TF-IDF_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4,\n",
       " 'learning_rate': 0.25,\n",
       " 'max_depth': 32,\n",
       " 'min_child_weight': 1,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.9361514243982431}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_params_v2 = space_eval(lgb_clf_params, lgb_opt_v2[0])\n",
    "optimum_params_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMCl_opt_v2 = LGBMClassifier(**optimum_params_v2)#random_state = 12345, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.4,\n",
       "               importance_type='split', learning_rate=0.25, max_depth=32,\n",
       "               min_child_samples=20, min_child_weight=1, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=0.9361514243982431, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LGBMCl_opt_v2.fit(tf_idf_v2_train, toxic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7710974284679464"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(toxic_test,LGBMCl_opt.predict(tf_idf_v2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новая модель дает f1 меру `0.771` Уже чуть больше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 [Логистическая регрессия на эмбеддингах из BERT](#Content) <a id='2.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки подготовлены [здесь](#features_BERT)    \n",
    "Нам предстоит разделить нашу выборку на тестовый и тренировочный датасет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_BERT_train, features_BERT_test, target_BERT_train, target_BERT_test = (train_test_split(features_BERT,\n",
    "                                                                                                 toxic_comment_reduced.toxic,\n",
    "                                                                                                 test_size=0.2,\n",
    "                                                                                                 random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_BERT_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pnedviga\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=500, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_regression_BERT = LogisticRegression(class_weight = 'balanced',max_iter=500)\n",
    "log_regression_BERT.fit(features_BERT_train, target_BERT_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6700443318556049"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_BERT_test,log_regression_BERT.predict(features_BERT_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7184555984555986"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_BERT_train,log_regression_BERT.predict(features_BERT_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного обескуражен результатом. Потрачено гораздо больше времени и при этом получен худший результат на тектовой выборке. \n",
    "Пока остановлюсь на этом в своих изысканиях и подведу итоги в следующей главе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 [LightGBM на эмбеддингах из BERT](#Content) <a id='2.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры оставим без изменения для Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obj_BERT = HPOpt(features_BERT_train, \n",
    "            features_BERT_test,\n",
    "            target_BERT_train,\n",
    "            target_BERT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                                | 0/15 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  7%|███████████                                                                                                                                                          | 1/15 [00:06<01:28,  6.33s/trial, best loss: -0.6704438149197355]\u001b[A\n",
      " 13%|██████████████████████                                                                                                                                               | 2/15 [00:14<01:30,  6.94s/trial, best loss: -0.6794995187680462]\u001b[A\n",
      " 20%|█████████████████████████████████                                                                                                                                    | 3/15 [00:21<01:24,  7.02s/trial, best loss: -0.6794995187680462]\u001b[A\n",
      " 27%|████████████████████████████████████████████                                                                                                                         | 4/15 [00:28<01:16,  6.97s/trial, best loss: -0.6794995187680462]\u001b[A\n",
      " 33%|███████████████████████████████████████████████████████                                                                                                              | 5/15 [00:31<00:58,  5.84s/trial, best loss: -0.6794995187680462]\u001b[A\n",
      " 40%|██████████████████████████████████████████████████████████████████▍                                                                                                   | 6/15 [00:38<00:55,  6.20s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 47%|█████████████████████████████████████████████████████████████████████████████▍                                                                                        | 7/15 [00:43<00:45,  5.68s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 53%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                             | 8/15 [00:48<00:37,  5.35s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 9/15 [00:54<00:33,  5.65s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                       | 10/15 [00:57<00:24,  4.82s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 11/15 [01:00<00:17,  4.33s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                 | 12/15 [01:04<00:13,  4.37s/trial, best loss: -0.680568720379147]\u001b[A\n",
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 13/15 [01:11<00:10,  5.09s/trial, best loss: -0.6881516587677725]\u001b[A\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████           | 14/15 [01:16<00:04,  5.00s/trial, best loss: -0.6881516587677725]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:19<00:00,  5.33s/trial, best loss: -0.6881516587677725]\u001b[A\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb_opt_BERT = obj_BERT.process(fn_name='lgb_reg', \n",
    "                      space=lgb_para, \n",
    "                      trials=Trials(), \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получены следующие оптимальные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 23,\n",
       " 'min_child_weight': 6,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.856380335487004}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_params_BERT = space_eval(lgb_clf_params, lgb_opt_BERT[0])\n",
    "optimum_params_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMCl_BERT_opt = LGBMClassifier(**optimum_params_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=23,\n",
       "               min_child_samples=20, min_child_weight=6, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=0.856380335487004, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LGBMCl_BERT_opt.fit(features_BERT_train, target_BERT_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6850618458610848"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_BERT_test,LGBMCl_BERT_opt.predict(features_BERT_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно это не тот результат, за который боролись! `0.69` - совсем немного.\n",
    "\n",
    "Буду рад любым рекомендациям. Результат явно может быть выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [Выводы](#Content) <a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий обзор проведенной работы\n",
    "Что выполнили, сделали, рассчитали;\n",
    "1. Был открыт и изучен датасет. \n",
    "2. Мы рассмотрели различного рода преобразования текста в численные признаки:\n",
    "    - Создание n-грамм\n",
    "    - TF-IDF - частота употребления слова в совокупности с их важностью. \n",
    "    - Создание признаков (эмбеддингов) на базе BERT. \n",
    "3. Обучено два вида моделей на базе двух наборов признаков:\n",
    "    - LightGBM\n",
    "    - LogisticRegression\n",
    "4. Получены результаты классификации. Не все из них удовлетворяют заданным параметрам точности.    \n",
    "----\n",
    "### Главные выводы \n",
    "---\n",
    "Нашей задачей было подготовить и найти модель с устраивающим нас показателем точности.\n",
    "Подбеду тут одержала модель LightGBM на признаках TF-IDF с показателем `f1 = 0.771`. Следующей по качеству оказалась линейная регрессия с `f1 = 0.747`   \n",
    "\n",
    "Embeddings от BERT к сожалению и близко не дали таких результатов. По идее модель от BERT должна быть точнее, поскольку более корректно сопоставляет свойства (смысл слова) с численным представлением. \n",
    "\n",
    "Возможные причины провала эмбеддингов:\n",
    "- Малая выборка (сделан downsapling). Мало примеров. Есть большая вероятность, что понятие, встретившееся в тестовой выборке не встречалось в обучающей. \n",
    "- Несбалансированные классы в обучающей выборке.\n",
    "- Возможно повлияла размерность матрицы эмбеддингов - мы выполнили \"усечение\", что равносильно потере части информации. Соответственно осталось меньше \"пересекающихся эмбеддингов\" в тренировочной vs. тестовой выборке.    \n",
    "---\n",
    "### Рекомендации\n",
    "- Вероятно стоит получить embeddings для всего набора данных и уже после обучать модель. \n",
    "- Одновременно не следует \"резать\" признаки. У нас ограничения от модели составили 512 штук. Логичным шагом будет разбить каждое высказывание на предложения. \n",
    "- Следует выбирать подходящую по мощности машину или облачное решение для работы с текстами.\n",
    "---\n",
    "### Итог\n",
    "Для себя вынес следующее:\n",
    "в ML тексты пожалуй самое сложное, с чем можно работать. Думаю сложнее только декомпозиция записи речи. Там можно встретить ряды в совокупности с текстами. \n",
    "Вместе с тем тема очень интересная. Думаю поискать литературу по тематике.  \n",
    "***UPD 04.11.2020: купил книжечку 2020 года Орельена Жерона \"Прикладное машинное обучение...\" (2е издание)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
